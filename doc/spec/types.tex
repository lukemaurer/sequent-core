\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{hyperref}
\usepackage{ifthen}
\usepackage{preamble}

\begin{document}

\title{Typing Dual System FC and Sequent Core}

\maketitle

\section{Syntax}
\label{sec:syntax}

\begin{figure}[h]
\centering
\begin{gather*}
\begin{aligned}
  x, y, z, f, g, h, K &\in Var
  \\
  q, r &\in KontVar
  \\
  j &\in JumpVar
  \\
  a, b, T &\in TypeVar
  \\
  \cmd &\in Command
  &
  &::= \Let \abind \In c
  \Alt \cut{\tm}{\ko}
  \Alt \jump{\vect[n]{\tm_n}}{j}
  \\
  \tm &\in Term
  &
  &::= x
  \Alt \fn{\ann x \ty} \tm
  \Alt \compute{\ann q \ty} \cmd
  \Alt \lit
  \Alt \ty
  \Alt \cn
  \\
  \ko &\in Kont
  &
  &::= \ret q
  \Alt \app{\tm}{\ko}
  \Alt \koerce{\cn}{\ko}
  \Alt \caseas{\ann x \ty}{\vect[n]{\analt_n}}
  \\
  \abind &\in Binding
  &
  &::= \nonrec \bp
  \Alt \rec{\vect[n]{\bp_n}}
  \\
  \bp &\in BindPair
  &
  &::= \bindv{\ann x \ty}{\tm}
  \Alt \bindk{\ann j \ty}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
  \\
  \analt &\in Alternative
  &
  &::= \alt{\blank}{\cmd}
  \Alt \alt{K ~ \vect[n]{\ann{x_n}{\ty_n}}}{\cmd}
  \Alt \alt{\lit}{\cmd}
  \\
  \ty &\in Type
  &
  &::= \dots
  \\
  \ki &\in Kind
  &
  &::= \dots
  \\
  \cn &\in Coercion
  &
  &::= \dots
  \\
  \decl &\in Declaration
  &
  &::= \dots
  \\
  \pgm &\in Program
  &
  &::= \decl; \pgm
  \Alt \cmd
\end{aligned}
\end{gather*}
\caption{Syntax of Dual System FC}
\label{fig:dual-fc-syntax}
\end{figure}

The syntax for Dual System FC are shown in Figure~\ref{fig:dual-fc-syntax}.
Types, kinds, coercions, and declarations are unchanged by the sequent calculus
representation, so they are elided here.  Note that data constructors, written
$K$, are treated as a special sort of variable in the syntax, and additionally
type constructors, written $T$, are treated as a special sort of type variable.
We use some conventional shorthands to make programs easier to read:
\begin{itemize}
\item If the type annotations on variables, \emph{ie} the $\ty$ in $\ann x \ty$,
  are not important to a particular example, we will often omit them.

\item The function call constructor $\app{\blank}{\blank}$ associates to the
  right, so $\app{1}{\app{2}{\app{3}{\ret q}}}$ is the same as
  $\app{1}{(\app{2}{(\app{3}{\ret q})})}$.  Similarly, coercion continuations
  associate to the right as well, so that
  $\koerce{\cn_1}{\koerce{\cn_2}{\ret q}}$ is the same as
  $\koerce{\cn_1}{(\koerce{\cn_2}{\ret q})}$.  Both function calls and coercions
  share the same precedence and may be intermixed, so that
  $\app{1}{\koerce{\cn_2}{\app{3}{\ret q}}}$ is the same as
  $\app{1}{(\koerce{\cn_2}{(\app{3}{\ret q})})}$.

\item We will not always write the binding variable $\ann x \ty$ in the case
  continuation $\caseas{\ann x \ty}{\vect[n]{\analt_n}}$ when it turns out that
  $x$ is never referenced in $\alts$ or $\alts$, instead writing
  $\Case\vect[n]{\analt_n}$.  If instead $\ann x \ty$ is only referenced in the
  default alternative $\alt{\blank}{\cmd}$ in $\vect[n]{\analt_n}$, we will
  prefer to write $\ann x \ty$ in place of the wildcard $\blank$ pattern.  This
  often arises in a case continuation with \emph{only} a default alternative,
  $\caseas{\ann x \ty}{\alt{\blank}{\cmd}}$, which we write as the shortened
  $\Case\alt{\ann x \ty}{\cmd}$.
\end{itemize}

\section{Scope and exit analysis}
\label{sec:scope-analysis}

\begin{figure}
\centering
\begin{gather*}
\begin{aligned}
  \Gamma \in& Environment
  &
  &::= \mt
  \Alt \Gamma, x
  \Alt \Gamma, a
  \\
  \Delta \in& KoEnvironment
  &
  &::= q
  \Alt \Delta, j
\end{aligned}
\end{gather*}

Term scoping: $\scopetm{\Gamma}{\tm}$
\begin{gather*}
  \infer
  {\scopetm{\Gamma}{x}}
  {x \in \Gamma}
  \qquad
  \axiom{\scopetm{\Gamma}{\lit}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\compute q \cmd}}
  {\scopecmd{\Gamma}{q}{\cmd}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\cn}}
  {\scopecn{\Gamma}{\cn}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\ty}}
  {\scopety{\Gamma}{\ty}}
  \\\\
  \infer
  {\scopetm{\Gamma}{\fn x \tm}}
  {\scopetm{\Gamma, x}{\tm}}
  \qquad
  \infer
  {\scopetm{\Gamma}{\fn a \tm}}
  {\scopetm{\Gamma, a}{\tm}}
\end{gather*}

Continuation scoping: $\scopeko{\Gamma}{\Delta}{\ko}$
\begin{gather*}
  \infer
  {\scopeko{\Gamma}{\Delta}{\ret q}}
  {q \in \Delta}
  \qquad
  \infer
  {\scopeko{\Gamma}{\Delta}{\app{\tm}{\ko}}}
  {
    \scopetm{\Gamma}{\tm}
    &
    \scopeko{\Gamma}{\Delta}{\ko}
  }
  \qquad
  \infer
  {\scopeko{\Gamma}{\Delta}{\app{\ty}{\ko}}}
  {
    \scopety{\Gamma}{\ty}
    &
    \scopeko{\Gamma}{\Delta}{\ko}
  }
  \\\\
  \infer
  {\scopeko{\Gamma}{\Delta}{\koerce{\cn}{\ko}}}
  {
    \scopecn{\Gamma}{\cn}
    &
    \scopeko{\Gamma}{\Delta}{\ko}
  }
  \qquad
  \infer
  {\scopeko{\Gamma}{\Delta}{\caseas{x}{\vect[n]{\analt_n}}}}
  {
    \vect[n]{
      \scopealt{\Gamma, x}{\Delta}{\analt_n}
    }
  }
\end{gather*}

Command scoping: $\scopecmd{\Gamma}{\Delta}{\cmd}$
\begin{gather*}
  \infer
  {\scopecmd{\Gamma}{\Delta}{\Let \abind \In \cmd}}
  {
    \scopebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}
    &
    \scopecmd{\Gamma,\Gamma'}{\Delta,\Delta'}{\cmd}
  }
  \\\\
  \infer
  {\scopecmd{\Gamma}{\Delta}{\cut{\tm}{\ko}}}
  {
    \scopetm{\Gamma}{\tm}
    &
    \scopeko{\Gamma}{\Delta}{\ko}
  }
  \qquad
  \infer
  {\scopecmd{\Gamma}{\Delta}{\jump{\vect[n]{\tm_n}}{j}}}
  {
    j \in \Delta
    &
    \vect[n]{
      \scopetm{\Gamma}{\tm_n}
    }
  }
\end{gather*}


\emph{Further rules for $\scopety{\Gamma}{\ty}$, $\scopeki{\Gamma}{\ki}$, and
  $\scopecn{\Gamma}{\cn}$}
\caption{Scope and exit analysis for terms, continuations, and commands}
\label{fig:scoping-rules}
\end{figure}

\begin{figure}
\centering

Binding scoping: $\scopebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}$ and
$\scopebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}$
\begin{gather*}
  \infer
  {\scopebind{\Gamma}{\Delta}{\nonrec{\bp}}{\Gamma'}{\Delta'}}
  {\scopebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}}
  \qquad
  \infer
  {\scopebind{\Gamma}{\Delta}{\rec{\vect[n]{\bp_n}}}{\Gamma'}{\Delta'}}
  {
    \vect[n]{
      \scopebp{\Gamma,\Gamma'}{\Delta,\Delta'}{\bp_n}{\Gamma'_n}{\Delta'_n}
    }
    &
    \Gamma' = \vect[n]{\Gamma'_n}
    &
    \Delta' = \vect[n]{\Delta'_n}
  }
\end{gather*}
\begin{gather*}
  \infer
  {\scopebp{\Gamma}{\Delta}{\bindv{x}{\tm}}{x}{\mt}}
  {\scopetm{\Gamma}{\tm}}
  \qquad
  \infer
  {\scopebp{\Gamma}{\Delta}{\bindk{j}{\fnk{\vect[n]{x_n}}\cmd}}{\mt}{j}}
  {\scopecmd{\Gamma,\vect[n]{x_n}}{\Delta}{\cmd}}
\end{gather*}

Alternative scoping: $\scopealt{\Gamma}{\Delta}{\analt}$
\begin{gather*}
  \infer
  {\scopealt{\Gamma}{\Delta}{\alt{\blank}{\cmd}}}
  {\scopecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {\scopealt{\Gamma}{\Delta}{\alt{\lit}{\cmd}}}
  {\scopecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {\scopealt{\Gamma}{\Delta}{\alt{K ~ \vect[i]{x_i}}{\cmd}}}
  {\scopecmd{\Gamma,\vect[i]{x_i}}{\Delta}{\cmd}}
\end{gather*}

Program scoping: $\scopepgm{\Gamma}{\Delta}{\pgm}$
\begin{gather*}
  \infer
  {\scopepgm{\Gamma}{\Delta}{\decl; \pgm}}
  {
    \scopedecl{\Gamma}{\decl}{\Gamma'}
    &
    \scopepgm{\Gamma,\Gamma'}{\Delta}{c}
  }
  \qquad
  \infer
  {\scopepgm{\Gamma}{\Delta}{\cmd}}
  {\scopecmd{\Gamma}{\Delta}{\cmd}}
\end{gather*}

\emph{Further rules for $\scopedecl{\Gamma}{\decl}{\Gamma'}$.}
\caption{Scope and exit analysis for bindings, alternatives, and
  programs}
\label{fig:scoping-rules-binds}
\end{figure}

The scoping rules for variables are shown in Figures~\ref{fig:scoping-rules} and
\ref{fig:scoping-rules-binds}, where the rules for scoping inside types, kinds,
coercions, and declarations are elided.  Continuation and jump variables are
treated differently from the other sorts of variables, being placed in a
separate environment $\Delta$, in order to prevent non-functional uses of
control flow.

Besides the normal rules for checking variable scope, these rules effectively
also perform an \emph{exit analysis} on a program (bindings, terms, commands,
\emph{etc}).  The one major restriction that we enforce is that terms must
always have a \emph{unique} exit point and cannot jump outside their scope.  The
intuition is:
\begin{quote}
  Terms cannot contain any references to free continuation or jump variables.
\end{quote}
This restriction makes sure that $\lambda$-abstractions cannot close over
continuation variables available from its context, so that bound continuations
and jump variables do not escape through a returned $\lambda$-abstraction.
Thus, all the jump variables used within a $\lambda$-abstraction must be local
to that $\lambda$-abstraction.  Additionally, in all computations
$\comp{r}\cmd$, the underlying command $c$ has precisely one unique exit point,
$r$, which names the returned result of the computation.  Any jump variables
referenced inside of $c$ must be local to the term itself, and not refer to its
enclosing scope.

If the command $c$ inside the well-scoped term $\comp{r}\cmd$ stops execution
with some value $V$ sent to some continuation variable $q$, then we know that:
\begin{itemize}
\item $q$ must be equal to $r$, due to the fact that $r$ is the only allowable
  free continuation variable inside of $\cmd$, and
\item $r$ does not appear free inside the resulting value $V$, again due to the
  scoping rules for continuation variables inside of a command.
\end{itemize}
In the simple case, this means execution of the term $\comp{r}\cmd$ yields
$\comp{r}\cut{V}{\ret r}$, which $\eta$-reduces to just the value $V$ by the
previously mentioned reasoning.  Thus, evaluating a term always results in a
unique value.

Notice that these scoping rules, while not very complex, still manage to tell us
something about the expressive capabilities of the language.  For example, we
syntactically permit value and continuation bindings within the same recursive
block, but can they mutually call one another?  It turns out that these scoping
rules disallow any sort of interesting mutual recursion between terms and
continuations because terms are \emph{prevented} from referencing continuations
within their surrounding (or same) binding environment.

For example, in a simple case where we have the recursive bindings:
\begin{gather*}
  \rec{\bindv{f}{\fn x \tm}; \bindk{j}{\fnk{y}\cmd}}
\end{gather*}
then by the scoping rules, $q$ may call $f$ through $\cmd$, but $f$ cannot jump
back to $j$ in $\tm$ because $\fn x tm$ cannot contain the free reference to
$j$.  Therefore, since there is no true mutual recursion between both $f$ and
$j$, we can break the recursive bindings into two separate blocks with the
correct scope:
\begin{gather*}
  \rec{\bindv{f}{\fn x \tm}}; \rec{\bindk{j}{\fnk{y}\cmd}}
\end{gather*}
While we do not syntactically enforce this restriction, it would not case any
loss of expressiveness.  Indeed, we could further the partitions into
\begin{enumerate}
\item first, the list of value bindings, and
\item second, the list of continuation bindings,
\end{enumerate}
since continuations can refer to previously bound terms but not vice versa.
However, we do not make this distinction here.

\section{Type checking}
\label{sec:typing}

\begin{figure}
\centering
\begin{align*}
  \Gamma \in& Environment
  &
  &::= \mt
  \Alt \Gamma, x : \ty
  \Alt \Gamma, a : \ki
  \\
  \Delta \in& KoEnvironment
  &
  &::= q : \ty
  \Alt \Delta, j : \ty
\end{align*}

Term typing: $\typetm{\Gamma}{\tm}{\ty}$
\begin{gather*}
  \infer
  {\typetm{\Gamma}{x}{\ty}}
  {x:\ty \in \Gamma}
  \qquad
  \infer
  {\typetm{\Gamma}{\lit}{\ty}}
  {\ty = literalType(\lit)}
  \qquad
  \infer
  {\typetm{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}}
  {\typecn{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}}
  \qquad
  \infer
  {\typetm{\Gamma}{\ty}{\ki}}
  {\typety{\Gamma}{\ty}{\ki}}
  \\\\
  \infer
  {\typetm{\Gamma}{\compute{\ann q \ty} \cmd}{\ty}}
  {\typecmd{\Gamma}{q:\ty}{\cmd}}
  \qquad
  \infer
  {\typetm{\Gamma}{\fn{\ann x {\ty_1}} \tm}{\ty_1 \to \ty_2}}
  {\typetm{\Gamma, x:\ty_1}{\tm}{\ty_2}}
  \qquad
  \infer
  {\typetm{\Gamma}{\fn{\ann a \ki} \tm}{\forall \ann a \ki. \ty}}
  {\typetm{\Gamma, a:\ki}{\tm}{\ty}}
\end{gather*}

Continuation typing: $\typeko{\Gamma}{\Delta}{\ko}{\ty}$
\begin{gather*}
  \infer
  {\typeko{\Gamma}{\Delta}{\ret q}{\ty}}
  {q:\ty \in \Delta}
  \qquad
  \infer
  {\typeko{\Gamma}{\Delta}{\app{\tm}{\ko}}{\ty_1 \to \ty_2}}
  {
    \typetm{\Gamma}{\tm}{\ty_1}
    &
    \typeko{\Gamma}{\Delta}{\ko}{\ty_2}
  }
  \qquad
  \infer
  {\typeko{\Gamma}{\Delta}{\app{\ty_1}{\ko}}{\forall \ann a \ki. \ty_2}}
  {
    \typety{\Gamma}{\ty_1}{\ki}
    &
    \typeko{\Gamma}{\Delta}{\ko}{\ty_2\subst{a}{\ty_1}}
  }
  \\\\
  \infer
  {\typeko{\Gamma}{\Delta}{\koerce{\cn}{\ko}}{\ty_1}}
  {
    \typecn{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}
    &
    \typeko{\Gamma}{\Delta}{\ko}{\ty_2}
  }
  \qquad
  \infer
  {\typeko{\Gamma}{\Delta}{\caseas{\ann x \ty}{\vect[n]{\analt_n}}}{\ty}}
  {
    \vect[n]{
      \typealt{\Gamma, x:\ty}{\Delta}{\analt_n}{\ty}
    }
  }
\end{gather*}

Command typing: $\typecmd{\Gamma}{\Delta}{\cmd}$
\begin{gather*}
  \infer
  {\typecmd{\Gamma}{\Delta}{\Let \abind \In \cmd}}
  {
    \typebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}
    &
    \typecmd{\Gamma,\Gamma'}{\Delta,\Delta'}{\cmd}
  }
  \\\\
  \infer
  {\typecmd{\Gamma}{\Delta}{\cut{\tm}{\ko}}}
  {
    \typetm{\Gamma}{\tm}{\ty}
    &
    \typeko{\Gamma}{\Delta}{\ko}{\ty}
  }
  \qquad
  \infer
  {\typecmd{\Gamma}{\Delta}{\jump{\vect[n]{\tm_n}}{j}}}
  {
    j:\ty \in \Delta
    &
    \typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty}
  }
\end{gather*}

\emph{Further rules for $\typety{\Gamma}{\ty}{\ki}$,
  $\typeki{\Gamma}{\ki}{\delta}$, and
  $\typecn{\Gamma}{\cn}{\eqty{\ty_1}{\ty_2}}$.}
\caption{Type checking rules for terms, continuations, and commands}
\label{fig:typing-rules}
\end{figure}

\begin{figure}
\centering

Binding typing: $\typebind{\Gamma}{\Delta}{\abind}{\Gamma'}{\Delta'}$ and
$\typebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}$
\begin{gather*}
  \infer
  {\typebind{\Gamma}{\Delta}{\nonrec{\bp}}{\Gamma'}{\Delta'}}
  {\typebp{\Gamma}{\Delta}{\bp}{\Gamma'}{\Delta'}}
  \qquad
  \infer
  {
    \typebind{\Gamma}{\Delta}{\rec{\vect[n]{\bp_n}}}{\Gamma'}{\Delta'}
  }
  {
    \vect[n]{
      \typebp
      {\Gamma,\Gamma'}{\Delta,\Delta'}
      {\bp_n}
      {\Gamma'_n}{\Delta'_n}
    }
    &
    \Gamma' = \vect[n]{\Gamma'_n}
    &
    \Delta' = \vect[n]{\Delta'_n}
  }
\end{gather*}
\begin{gather*}
  \infer
  {\typebp{\Gamma}{\Delta}{\bindv{\ann x \ty}{\tm}}{x:\ty}{\mt}}
  {\typetm{\Gamma}{\tm}{\ty}}
  \qquad
  \infer
  {
    \typebp
    {\Gamma}{\Delta}
    {\bindk{j}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}}
    {\mt}{j:\ty}
  }
  {\typejmpko{\Gamma}{\Delta}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}{\ty}}
\end{gather*}

Alternative typing: $\typealt{\Gamma}{\Delta}{\analt}{\ty}$
\begin{gather*}
  \infer
  {\typealt{\Gamma}{\Delta}{\alt{\blank}{\cmd}}{\ty}}
  {\typecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {\typealt{\Gamma}{\Delta}{\alt{\lit}{\cmd}}{\ty}}
  {
    \typetm{\Gamma}{\lit}{\ty}
    &
    \typecmd{\Gamma}{\Delta}{\cmd}
  }
  \\\\
  \infer
  {
    \typealt
    {\Gamma}{\Delta}
    {
      \alt{
        K ~ \vect[m']{\ann{b_{m'}}{\theta(\ki'_{m'})}} \vect[n]{\ann{x_{n}}{\theta(\ty_{n})}}
      }
      {\cmd}
    }
    {T ~ \vect[m]{\ty'_m}}
  }
  {
  \begin{aligned}[b]
    &
    K
    :
    \vect[m]{\forall\ann{a_m}{\ki_m}.} \vect[m']{\forall\ann{b_{m'}}{\ki'_{m'}}.}
      \vect[n]{\ty_{n} \to} T ~ \vect[m]{a_m}
    \in
    \Gamma
    \\
    &
    \typecmd
    {\Gamma, \vect[m']{b_{m'}:\theta(\ki'_{m'})}, \vect[n]{x_{n}:\theta(\ty_{n})}}
    {\Delta}
    {\cmd}
  \end{aligned}
    &
    \theta = \vect[m]{\subst{a_m}{\ty'_m}}
  }
\end{gather*}

Program typing: $\typepgm{\Gamma}{\Delta}{\pgm}$
\begin{gather*}
  \infer
  {\typepgm{\Gamma}{\Delta}{\decl; \pgm}}
  {
    \typedecl{\Gamma}{\decl}{\Gamma'}
    &
    \typepgm{\Gamma,\Gamma'}{\Delta}{\pgm}
  }
  \qquad
  \infer
  {\typepgm{\Gamma}{\Delta}{\cmd}}
  {\typecmd{\Gamma}{\Delta}{\cmd}}
\end{gather*}

\emph{Further rules for $\typedecl{\Gamma}{\decl}{\Gamma'}$.}
\caption{Type checking rules for bindings, alternatives, and programs}
\label{fig:typing-rules-binds}
\end{figure}

\begin{figure}
\centering

 Term argument typing: $\typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty}$
\begin{gather*}
  \axiom{\typejmptm{\Gamma}{}{\unitjmpty}}
  \qquad
  \infer
  {\typejmptm{\Gamma}{v', \vect[n]{\tm_n}}{\prodjmpty{\ty'}{\ty}}}
  {
    \typetm{\Gamma}{v'}{\ty'}
    &
    \typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty}
  }
  \\\\
  \infer
  {\typejmptm{\Gamma}{\ty', \vect[n]{\tm_n}}{\existjmpty{\ann a \ki}{\ty}}}
  {
    \typety{\Gamma}{\ty'}{\ki'}
    &
    \typejmptm{\Gamma}{\vect[n]{\tm_n}}{\ty\subst{a}{\ty'}}
  }
\end{gather*}

Parameterized continuation typing:
$\typejmpko{\Gamma}{\Delta}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}{\ty}$
\begin{gather*}
  \infer
  {\typejmpko{\Gamma}{\Delta}{\fnk{}\cmd}{\unitjmpty}}
  {\typecmd{\Gamma}{\Delta}{\cmd}}
  \qquad
  \infer
  {
    \typejmpko
    {\Gamma}{\Delta}
    {\fnk{\ann{y}{\ty'}~\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\prodjmpty{\ty'}{\ty}}
  }
  {
    \typejmpko
    {\Gamma, y:\ty'}{\Delta}
    {\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\ty}
  }
  \\\\
  \infer
  {
    \typejmpko
    {\Gamma}{\Delta}
    {\fnk{\ann{a}{\ki}~\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\existjmpty{\ann{a}{\ki}}\ty}
  }
  {
    \typejmpko
    {\Gamma, a:\ki}{\Delta}
    {\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
    {\ty}
  }
\end{gather*}

\caption{Type checking rules for parameterized continuations and their arguments}
\label{fig:typing-rules-jumps}
\end{figure}

The typing rules for Dual System FC are given in Figures~\ref{fig:typing-rules},
\ref{fig:typing-rules-binds}, and \ref{fig:typing-rules-jumps}.  The type of a
term classifies the results that it might produce, and the type of a
continuation classifies the results that it expects to consume.  Commands do not
have a type; they are just $\Ok$ to run.  The eventual result of a command is
returned through the unique return continuation variable available in its
environment.  Likewise, a program is a consistent block of code that is capable
of running, meaning that a program is a command that runs with respect to some
top-level declarations that introduce type information (data types, type
synonyms, and axioms).  The normal way to type-check a top-level program is
$\typepgm{\Gamma_0}{\tp:\ty}{\pgm}$, where $\Gamma_0$ specifies any primitive
types and values provided by the run-time environment, and $\tp:\ty$ is the
single, top-level exit path out of the program that expects a $\ty$ result.

Compared with System FC, more of the typing rules enjoy the
\emph{sub-formula property}, meaning that the types appearing in a premise above
the line of a rule appear somewhere below the line.  This is a natural
consequence of the sequent calculus as a logic, and was one of the primary
motivations for its original development.  The expected rules violating the
sub-formula property are the various \emph{cut} rules for commands that cancel
out arbitrary types, where the $\Let$-form of command allows us to perform
multiple cuts simultaneously.  The other interesting violators of note are:
\begin{itemize}
\item The rule for a polymorphic call-stack,
  $\app{\ty_1}{\ko} : \forall\ann{a}{\ki}.\ty_2$, which substitutes the
  specified type $\ty_1$ in for the variable $a$ in $\ty_2$ to get the type for
  the continuation.  This rule does not have the sub-formula property since
  $\ty_2\subst{a}{\ty_1}$ is a new type generated by the substitution.

  Since universal quantification is dual to existential quantification, the
  polymorphic call-stack is dual to the existential pair
  $(\tau_1,\tm) : \exists\ann{a}{\ki}.\tau_2$, and shares the same properties.
  In particular, $\app{\tau_1}{\ko}$ does not have a \emph{unique} type.  For
  example, given the continuation variable $r$ of type $Bool$, then the
  polymorphic call-stack $\app{Int}{\app{1}{\app{2}{\ret q}}}$ can be given the
  types $\forall \ann{a}{\star}. a \to a \to Bool$,
  $\forall \ann{a}{\star}. Int \to a \to Bool$, and so on.  So following the
  bottom-up preference of a sequent calculus presentation, it is easy to type
  check a polymorphic call-stack if we already know the type of function it
  expects, but in general it is hard to guess its type.

\item The rules for pattern matching on data types almost suffers from the same
  issue as for polymorphic call-stacks, due to substitution of the choice for
  polymorphic type variables.  However, the type annotations on variables bound
  by pattern matching already specify the specialized types, so the issue is
  avoided.

  Also note that the problem with existential pairs (or in general, existential
  data structures) mentioned in the previous point is avoided on the term side.
  This is because we do not represent data structures directly, as a
  fully-applied constructor like $(\ty, \tm)$, but rather we represent them
  indirectly as a constructor evaluated with a polymorphic call-stack,
  $\cut{(,)}{\app{\ty}{\app{\tm}{\ret q}}}$.  That means that all the troubles
  with checking user-defined existential data structures are contained solely in
  polymorphic call stacks.

\item The rule for coercion continuations, $\koerce{\cn}{\ko} : \ty_1$, in which
  the type of the continuation $\ko$ is hidden by the coercion.  Interestingly,
  the typing rules for casting resembles a special sort of function application,
  both with terms in System FC and continuations in Dual System FC.
\end{itemize}

Due to the difficulty of inferring the type of a polymorphic call-stack, the
type checking algorithm for Sequent Core reveals some asymmetrical bias of type
information.  As designed, reading the type off of a Sequent Core term is always
straightforward.  However, continuations may have several different types (due
to the polymorphic form of call stacks).  Instead, it is straightforward to
check if a continuation has a particular type.  Contrarily, the type of the term
arguments, $\Jump\vect{\tm}$, is easy to check but hard to infer, whereas the
type of the parameterized continuation $\fnk{\vect{x:\ty}}\cmd$ is easy to
infer.  This flow of type-inferring versus type-checking is captured in the
structure of a command and gives us a type checking algorithm:
\begin{itemize}
\item If the command is a $\Let$-binding, gather the types for the bound local
  variables and confirm that they are bound to well-typed terms and
  continuations.
\item If the command evaluates a term in the eye of a continuation,
  $\cut{\tm}{\ko}$, then read the type off of $\tm$, thereby confirming that it
  is well-typed, and check that $\ko$ has the same type.
\item If the command is a jump, $\jump{\vect{\tm}}{j}$, then read the type off
  of $j$, and check that $\Jump\vect{\tm}$ has the same type.
\end{itemize}
Determining the explicit flow of type information (from term to continuation or
continuation to term, depending on the form of the command) likely has something
to do with the traditional form of bi-directional type checking from the
literature.  Intuitively, the problem with inferring the type of polymorphic
call stacks is identical to the problem with inferring the general existential
tuples in the sequent calculus.  Thus, solving the problem with forall gives us
a solution for exists by duality in the sequent calculus.  Symmetry saves the
day!

\section{Semantics}
\label{sec:semantics}

Here we illustrate the semantics of Sequent Core programs in two forms:
\begin{itemize}
\item An equational theory of program transformations that a compiler might
  perform, in any order, to simplify a Sequent Core program.
\item A small-step, operational semantics that suggests how an interpreter might
  evaluate a Sequent Core program.
\end{itemize}
For simplicity, the presented semantics Sequent Core only accounts for
call-by-name evaluation and non-recursive bindings.  A more thorough semantics
would combine both call-by-name evaluation (for lifted types) and call-by-value
evaluation (which is necessary for unlifted types), along with recursive
$\Let$-bindings.

\subsection{Equational Theory}

First, we have the sequent equivalent of the usual $\beta$ and $\eta$ axioms for
functions:
\begin{align*}
  \cut{\fn{\ann{x}{\ty}} \tm}{\app{\tm'}{\ko}}
  &=
  \Let \nonrec{\bindv{\ann{x}{\ty}}{\tm'}} \In \cut{\tm}{\ko}
  &
  \Where
  x &\notin FV(\ko)
  \\
  \fn{\ann{x}{\ty}}\comp{\ann{r}{\ty'}} \cut{\tm}{\app{x}{\ret r}}
  &=
  \tm
  &
  \Where
  x &\notin FV(\tm)
  \\
  &&
  \tm &\neq \bot : \ty \to \ty'
\end{align*}
For the $\beta$ axiom, we simplify a $\lambda$-abstraction that is evaluated
with respect to a calling continuation by $\Let$-binding the argument and
evaluating the body in the return return continuation.  For the $\eta$ axiom, we
recognize that the functional abstraction
$\fn{x}\comp{r}\cut{f}{\app{x}{\ret r}}$ immediately delegates to $f$, so the
two are equivalent.  Note that, due to the presence of unconstrained,
polymorphic strictness (for example,\texttt{seq}), the $\eta$ axiom can only be
applied to terms of a function type that do not evaluate to bottom.  In other
words, the $\eta$ axiom only applies to terms that are already equal to a
$\lambda$-abstraction without the use of $\eta$.

Computation abstractions (that is, $\mu$-abstractions), also follow a form of
$\beta$ and $\eta$ axioms:
\begin{align*}
  \cut{\comp{r}\cmd}{\ko}
  &=
  \cmd\subst{r}{\ko}
  \\
  \comp{r}\cut{\tm}{\ret r}
  &=
  \tm
  &
  \Where
  r \notin FV(\tm)
\end{align*}
The $\beta$ axiom for a $\mu$-abstraction substitutes its entire continuation
for the bound continuation variable.  To avoid unnecessary code duplication, we
may only want to apply the $\beta$ axiom for $\mu$ when $k$ is small (and we can
force $k$ to be small by introducing auxiliary bindings naming arguments and
join points).  The $\eta$ axiom for a $\mu$-abstraction is more well-behaved
than the one for functions: it applies to \emph{any} term that does not
reference the bound continuation variable.

Case continuations have their own form of $\beta$ and $\eta$ axioms.  The
$\beta$ axiom for a case continuation selects the appropriate branch based on
the structure of input, in the case that it is given a literal or a constructed
data structure.
\begin{align*}
  \cut{\lit}{\caseas{\ann{x}{\ty}}{\dots;\alt{\lit}{\cmd};\dots}}
  &=
  \cmd\subst{x}{\lit}
\end{align*}
\begin{align*}
  &
  \cut
  {K}
  {\app
    {\vect[n]{\tm_n}}
    {\caseas{\ann{x}{\ty}}{
      \dots;\alt{K ~ \vect[n]{\ann{y_n}{\sty_n}}}{\cmd};\dots}}}
  \\
  &=
  \vect[n]{
  \Let
    \nonrec{\bindv{\ann{y_n}{\sty_n}}{\tm_n}}
  \In
  }
  \Let \nonrec{
    \bindv
    {\ann{x}{\ty}}
    {\comp{\ann{r}{\ty}}\cut{K}{\app{\vect[n]{y_n}}{\ret r}}}
  } \In
    \cmd
\end{align*}
The default alternative for a case continuation is selected when no other
alternative matches, and only applies when the continuation is given a weak-head
normal form (WHNF) as input.
\begin{align*}
  \cut{\tm'}{\app{\vect[n]{\tm_n}}{\caseas{\ann{x}{\ty}}{\dots;\alt{\blank}{\cmd}}}}
  &=
  \Let
    \nonrec{\bindv{\ann{x}{\ty}}{\tm''}}
  \In
    \cmd
  \\
  \Where
  \tm''
  &=
  \comp{\ann{r}{\ty}}\cut{\tm'}{\app{\vect[n]{\tm_n}}{\ret r}}
  \in
  WHNF
\end{align*}
Note that the list of arguments, $\vect[n]{\tm_n}$, can only be non-empty when
the head term $\tm$ is a constructor:
$\comp{\ann{r}{\ty}}\cut{K}{\app{\vect[n]{\tm_n}}{\ret r}}$ is a WHNF.  The
other possibility is that $\vect[n]{\tm_n}$ is empty and $\tm'$ is a literal or
a $\lambda$-abstraction.  We also have some extensional properties about case
continuations.  First, we may replace the name for the input with the pattern in
any alternative, or vice versa:
\begin{align*}
  \caseas{\ann{x}{\ty}}{\dots;\alt{\lit}{\cmd};\dots}
  &=
  \caseas{\ann{x}{\ty}}{\dots;\alt{\lit}{\cmd\subst{x}{\lit}};\dots}
\end{align*}
\begin{align*}
  &
  \caseas{\ann{x}{\ty}}{\dots;\alt{K ~ \vect[n]{\ann{y_n}{\sty_n}}}{\cmd};\dots}
  \\
  &=
  \caseas{\ann{x}{\ty}}{
    \dots;
    \alt
    {K ~ \vect[n]{\ann{y_n}{\sty_n}}}
    {\cmd\subst{x}{\comp{\ann{r}{\ty}}\cut{K}{\app{\vect[n]{y_n}}{r}}}};
    \dots
    }
\end{align*}
Second, we have a form of $\eta$ axiom for case continuations which eliminates
a redundant case analysis wrapped around another continuation.
\begin{align*}
  \caseas{\ann{x}{\ty}}{\vect[n]{\alt{\pat_n}{\cut{x}{\ko}}}}
  &=
  \ko
  &
  \Where{}
  \{x\} \cup BV(\vect[n]{\pat_n}) &\notin FV(\ko)
  ,
  \ko : \ty
\end{align*}
These extensionality laws are dual to the $\eta$ laws for $\lambda$- and
$\mu$-abstractions.  For instance, for pairs, we have the dual equality to
functional $\eta$:
\begin{align*}
  \Case\Of \alt{(\ann{x}{\ty_1},\ann{y}{\ty_2})}{\cut{(x,y)}{\ko}}
  &=
  \ko
  &
  \Where
  x,y &\notin FV(\ko)
  ,
  \ko : (\ty_1, \ty_2)
\end{align*}
Furthermore, for a case continuation with only a default alternative, we have
the dual equality to $\mu$ $\eta$:
\begin{align*}
  \Case\Of \alt{x}{\cut{x}{\ko}}
  &=
  \ko
  &
  \Where
  x &\notin FV(\ko)
\end{align*}

The $\Let$ form of commands substitute their bindings into the underlying
command.  First, we interpret each type of binding pair as a substitution:
\begin{align*}
  \subp{\bindv{x}{\tm}} &= \subst{x}{\tm}
  \\
  \subp{\bindk{j}{\fnk{\vect[n]{x_n}}\cmd}}
  &=
  \subst{\vect[n]{\Let \nonrec{\bindv{x_n}{\tm_n}} \In} \cmd}{\jump{\vect[n]{\tm_n}}{j}}
\end{align*}
The substitution of a value binding is the normal capture-avoiding substitution
of terms.  The substitution of a continuation binding  Then a $\Let$-binding command substitutes its binding pair:
\begin{align*}
  \Let \nonrec{\bp} \In \cmd
  &=
  \cmd\subp{\bp}
\end{align*}

We now consider axioms for pushing casts around the program.  First, we
introduce syntactic sugar for casting the output of a term via the dual form of
casting the input of its continuation:
\begin{align*}
  \coerce{\tm}{\cn} &= \comp{r}\cut{\tm}{\koerce{\cn}{\ret r}}
\end{align*}
A sequence of casts can be combined into a single cast of the composed coercion:
\begin{align*}
  \koerce{\cn}{(\koerce{\cn'}{\ko})}
  &=
  \koerce{(\compcn{\cn}{\cn'})}{\ko}
\end{align*}
Casting a coercion itself can be reduced to a modified coercion that pipes the
type conversions together:
\begin{align*}
  \cut{\cn}{\koerce{\cn'}{\ko}}
  &=
  \cut{\compcn{\cn'_0}{\compcn{\cn}{\cn'_1}}}{\ko}
  &
  \Where
  \cn &: \eqty{\ty_0}{\ty_1}
  \\ &&
  \cn' &: \eqty{(\eqty{\ty_0}{\ty_1})}{(\eqty{\ty'_0}{\ty'_1})}
  \\ &&
  \cn'_0 &: \eqty{\ty'_0}{\ty_0} = \sym(\nth_0 \cn')
  \\ &&
  \cn'_1 &: \eqty{\ty_1}{\ty'_1} = \nth_1 \cn'
\end{align*}
In general, we will always push casts down structures.  For coercions between
function type, this means pushing the cast down a call-stack:
\begin{align*}
  \koerce{\cn}{(\app{\tm}{\ko})}
  &=
  \app{(\coerce{\tm}{\cn_0})}{(\koerce{\cn_1}{\ko})}
  \\
  \Where
  \cn &: \eqty{(\ty_0 \to \ty_1)}{(\ty_0' \to \ty_1')}
  \\
  % \tm &\notin Type \cup Coercion
  \tm &\notin Type
  \\
  \cn_0 &: \eqty{\ty'_0}{\ty_0} = \sym (\nth_0 \cn)
  \\
  \cn_1 &: \eqty{\ty_1}{\ty'_1} = \nth_1 \cn
\end{align*}
Similarly, casts down polymorphic call-stacks for coercions between universally
quantified types:
\begin{align*}
  \koerce{\cn}{(\app{\ty}{\ko})}
  &=
  \app{\ty}{(\koerce{\cn'}{\ko})}
  \\
  \Where
  \cn &: \eqty{\forall a:\ki. \ty'_1}{\forall a:\ki. \ty'_2}
  \\
  \cn' &: \eqty{\ty'_1\subst{a}{\ty}}{\ty'_2\subst{a}{\ty}} = \instancn{\cn}{\ty}
\end{align*}
Casting a literal is a no-op, so long as the coercion represents the reflexive
equality of the literal type with itself:
\begin{align*}
  \cut{\lit}{\koerce{\cn}{\ko}}
  &=
  \cut{\lit}{\ko}
  &
  \Where
  \cn &: \eqty{\ty}{\ty}
  ,
  \lit : \ty
\end{align*}
The most complicated push axiom is for data types.  Here, we push a cast on a
constructed data structure into its sub-terms:
\begin{align*}
  \cut
  {K}
 {\app
   {\vect[m]{\ty_{m}}}
   {\app
     {\vect[m']{\sty_{m'}}}
     {\app
       {\vect[n]{\tm_n}}
       {\koerce{\cn}{\ko}}}}}
  &=
  \cut
  {K}
  {\app
    {\vect[m]{\ty'_{m}}}
    {\app
      {\vect[m']{\sty_{m'}}}
      {\app
        {\vect[n]{(\coerce{\tm_n}{\cn'_n})}}
        {\ko}}}}
  \\
  \Where
  \cn &: \eqty{T ~ \vect[m]{\ty_m}}{T ~ \vect[m]{\ty'_m}}
  \\
  K
  &:
  \ty_K
  =
  \vect[m]{\forall a_{m}:\ki_{m}.}
  \vect[m']{\forall b_{m'}:\ki'_{'m}.}
  \vect[n]{\sty'_n \to} T ~ \vect[m]{a_{m}}
  \\
  \cn'_i
  &:
  \eqty
  {(\sty'_i\vect[m]{\subst{a_{m}}{\ty_m}}\vect[m']{\subst{b_{m'}}{\sty_{m'}}})}
  {(\sty'_i\vect[m]{\subst{a_{m}}{\ty'_m}}\vect[m']{\subst{b_{m'}}{\sty_{m'}}})}
  \\
  \cn'_i &= \ntharg_i(\idcn{\ty_K} ~ \vect[m]{\nth_{m} \cn} ~ \vect[m']{\idcn{\sty_{m'}}})
\end{align*}
Part of the complication is the different behavior of universally and
existentially quantified types in the structure.  The universally quantified
types, $\vect[m]{\ty_{m}}$, are replaced the type of the coercion, representing
a change in the externally visible type of the structure itself.  The
existentially quantified types, $\vect[m']{\sty_{m'}}$, don't change at all, as
they are a hidden internal part of the structure.  The value sub-components of
the structure, $\vect[n]{\tm_n}$, are get cast by the new coercions
$\vect[n]{\cn'_n}$.  These new coercions are formed by selecting the matching
argument type from the type of the constructor, and replacing the original
universally quantified types with the coerced ones.

\subsection{Operational Semantics}

Since we do not consider recursive bindings, the reduction steps in operational
semantics of Sequent Core always apply the to the top of a command.  The
reductions of non-recursive $\Let$-bindings, $\mu$-abstractions, and
$\lambda$-abstractions are:
\begin{align*}
  \Let \nonrec{\bp} \In \cmd
  &\sred
  \cmd\subp{\bp}
\end{align*}
\begin{align*}
  \cut{\comp{r}\cmd}{\ko}
  &\sred
  \cmd\subst{r}{\ko}
\end{align*}
\begin{align*}
  \cut{\fn x \tm}{\app{\tm'}{\ko}}
  &\sred
  \cut{\tm\subst{x}{\tm'}}{\ko}
\end{align*}

For the purpose of the simplicity of the operational semantics, we assume fully
saturated constructors, so an unapplied constructor
$K : \vect[m]{\forall\ann{a_{m}}{\ki_{m}}}\vect[n]{\ty_{n}\to}\ty'$ is
represented as
$
\vect[m]{\fn{a_{m}}}\vect[n]{\fn{x_{n}}}\comp{r}
  \cut{K ~ \vect[m]{a_{m}} ~ \vect[n]{x_{n}}}{\ret r}
$.
The constructed form of term greatly simplifies the presentation of the
operational semantics for case continuations.  The rules for reducing case
continuations are:
\begin{align*}
  \cut{\lit}{\caseas{x}{\dots;\alt{\lit}{\cmd};\dots}}
  &\sred
  \cmd\subst{x}{\lit}
  \\
  \cut
  {K ~ \vect[m]{\ty_{m}} ~ \vect[n]{\tm_{n}}}
  {\caseas{x}{\dots;\alt{K ~ \vect[n]{y_n}}{\cmd};\dots}}
  &\sred
  \cmd
  \subst{x}{(K ~ \vect[m]{\ty_{m}} ~ \vect[n]{\tm_{n}})}
  \vect[n]{\subst{y_n}{\tm_n}}
  \\
  \cut
  {\whnf}
  {\caseas{x}{\dots;\alt{\blank}{\cmd}}}
  &\sred
  \cmd\subst{x}{\whnf}
\end{align*}
Note that the $\whnf$ in the last rule stands for a WHNF, which is either a
literal, a $\lambda$-abstraction, or a constructed term $K ~ \vect[n]{\tm_n}$.

Finally, we have the reductions which push casts out of the way of $\beta$
reductions at the last moment.  For function and type abstractions, we have:
\begin{align*}
  \cut{\fn x \tm}{\koerce{\cn}{(\app{\tm'}{\ko})}}
  &\sred
  \cut
  {\fn x \tm}
  {\app{(\coerce{\tm'}{\sym(\nth_0 \cn)})}{(\koerce{\nth_1 \cn}{\ko})}}
  &
  \Where
  \tm' &\notin Type
  \\
  \cut{\fn a \tm}{\koerce{\cn}{(\app{\ty}{\ko})}}
  &\sred
  \cut
  {\fn a \tm}
  {\app{\ty}{(\koerce{\instancn{\cn}{\ty}}{\ko})}}
\end{align*}
For a matching literal case, we have:
\begin{align*}
  \cut{\lit}{\koerce{\cn}{\caseas{x}{\dots;\alt{\lit}{\cmd};\dots}}}
  &\sred  
  \cut{\lit}{\caseas{x}{\dots;\alt{\lit}{\cmd};\dots}}
\end{align*}
For a matching constructed term, we have:
\begin{align*}
  &
  \cut
  {K ~ \vect[m]{\ty_{m}} ~ \vect[m']{\sty_{m'}} ~ \vect[n]{\tm_{n}}}
  {\koerce{\cn}{\caseas{x}{\dots;\alt{K ~ \vect[m']{b_{m'}} \vect[n]{y_{n}}}{\cmd};\dots}}}
  \\
  &\sred
  \cut
  {K ~ \vect[m]{\ty'_{m}} ~ \vect[m']{\sty_{m'}} ~ \vect[n]{(\coerce{\tm_{n}}{\cn'_n})}}
  {\caseas{x}{\dots;\alt{K ~ \vect[m']{b_{m'}} \vect[n]{y_{n}}}{\cmd};\dots}}
  \\
  &\qquad
  \Where
  \cn : \eqty{T ~ \vect[m]{\ty_{m}}}{T ~ \vect[m]{\ty'_{m}}}
  \\
  &\qquad\phantom{\Where}
  K 
  :
  \vect[m]{\forall\ann{a_{m}}{\ki_{m}}.}
  \vect[m']{\forall\ann{b_{m'}}{\ki'_{m'}}.}
  \vect[n]{\sty'_{n} \to} T ~ \vect[m]{a_{m}}
  \\
  &\qquad\phantom{\Where}
  \cn'_i = \sty'_i\vect[m]{\subst{a_{m}}{\nth_m \cn}}\vect[m']{\subst{b_{m'}}{\idcn{\sty_{m'}}}}
\end{align*}
The default alternative of a case continuation doesn't have anything to do with
a particular type, so we can't really push the cast anywhere.  Instead, we
include it with the substitution in another form of the default case reduction:
\begin{align*}
  \cut{\whnf}{\koerce{\cn}{\caseas{x}{\dots;\alt{\blank}{\cmd}}}}
  &\sred
  \cmd\subst{x}{\coerce{\whnf}{\cn}}
\end{align*}
% \begin{align*}
%   &
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut
%     {K}
%     {\app
%       {\vect[m]{\ty_{m}}}
%       {\app
%         {\vect[m']{\sty_{m'}}}
%         {\app
%           {\vect[n']{\tm_{n'}}}
%           {\koerce
%             {\cn}
%             {\caseas{x}{\dots;\alt{K ~ \vect[n']{y_{n'}}}{\cmd};\dots}}}}}}
%   \end{aligned}
%   \\
%   &\sred
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut
%     {K}
%     {\app
%       {\vect[m]{\ty'_{m}}}
%       {\app
%         {\vect[m']{\sty_{m'}}}
%         {\app
%           {\vect[n']{(\coerce{\tm_{n'}}{\cn'_{n'}})}}
%           {\caseas{x}{\dots;\alt{K ~ \vect[n']{y_{n'}}}{\cmd};\dots}}}}}
%   \end{aligned}
%   \\
%   &
%   \Where
%   \cn : \eqty{T ~ \vect[m]{\ty_{m}}}{T ~ \vect[m]{\ty'_{m}}}
%   \\
%   &\phantom{\Where}
%   K : \ty_K
%   \\
%   &\phantom{\Where}
%   \cn_i = \arg_i(\idcn{\ty_K} ~ \vect[m]{\nth_{m} \cn} ~ \vect[m']{\idcn{\sty_{m'}}})
% \end{align*}
% \begin{align*}
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut
%     {\tm'}
%     {\app{\vect[m]{\tm_{m}}}{\koerce{\cn}{\caseas{x}{\dots;\alt{\blank}{\cmd}}}}}
%   \end{aligned}
%   &\sred
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cmd\subst{x}{\tm''}
%   \end{aligned}
%   \\
%   \Where
%   \tm''
%   &=
%   \comp{r}\cut{\tm'}{\app{\vect[m]{\tm_{m}}}{\koerce{\cn}{\ret r}}}
%   \text{ is a WHNF}
% \end{align*}

And finally, when we have two casts in a row, we merge them into a single cast
of the composed coercion:
\begin{align*}
  \cut{\V}{\koerce{\cn}{\koerce{\cn'}{\ko}}}
  &\sred
  \cut{\V}{\koerce{\compcn{\cn}{\cn'}}{\ko}}
\end{align*}

% \begin{align*}
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut{K}{\app{\vect[m]{\tm_m}}{\koerce{\cn}{(\app{\tm'}{\ko})}}}
%   \end{aligned}
%   &\sred  
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut
%     {K}
%     {\app
%       {\vect[m]{\tm_m}}
%       {\app
%         {(\coerce{\tm'}{\sym(\nth_0 \cn)})}
%         {(\koerce{\nth_1 \cn}{\ko})}}}
%   \end{aligned}
%   \\
%   \Where
%   m &< arity(K)
%   \\
%   \tm' &\notin Type
%   \\
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut{K}{\app{\vect[m]{\tm_m}}{\koerce{\cn}{(\app{\ty}{\ko})}}}
%   \end{aligned}
%   &\sred  
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut
%     {K}
%     {\app
%       {\vect[m]{\tm_m}}
%       {\app{\ty}{(\koerce{\cn ~ \ty}{\ko})}}}
%   \end{aligned}
%   \\
%   \Where
%   m &< arity(K)
%   \\
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut{K}{\app{\vect[m]{\tm_m}}{\koerce{\cn}{\koerce{\cn'}{\ko}}}}
%   \end{aligned}
%   &\sred
%   \begin{aligned}[b]
%     &
%     \Let \rec{\vect[n]{\bp_n}} \In
%     \\ &
%     \cut{K}{\app{\vect[m]{\tm_m}}{\koerce{\compcn{\cn}{\cn'}}{\ko}}}
%   \end{aligned}
%   \\
%   \Where
%   m &< arity(K)
% \end{align*}

\section{Translation}
\label{sec:translation}

An important aspect of Sequent Core is that it can be translated both to and
from Core, which has some benefits:
\begin{itemize}
\item We are sure that Sequent Core are at least as expressive as Core, so that
  it can represent every Core program and type.
\item We are sure that Sequent Core is not \emph{more} expressive than Core,
  which is a prevailing concern since the sequent calculus (like
  continuation-passing style) is a natural setting for first-class control.  We
  don't want to introduce unusual control flow that can't be represented at
  least somewhat directly in Core.
\item The compiler can process a program represented in Core for a bit, then
  represented in Sequent Core, then back in Core again.  This capability is what
  makes our use of the plugin architecture possible, so that we can add Sequent
  Core to GHC without modifying GHC itself!
\end{itemize}

\subsection{From Core to Sequent Core}

Let's start with the simplest translation of Core into Sequent Core: a
compositional translation of Core expressions into Sequent Core terms,
$Seq\trans{\expr}$.  Translating apparent%
\footnote{We say ``apparent'' because the structures of a data type are not
  expressed directly in Core, but are written as a chain of function
  applications with a constructor (a special sort of variable identifier) at the
  head.  Since $K~x~y~z$ ``looks like'' a function application at first glance,
  it is apparently not a value even though in actuality it is.}
 %
values from Core is not hard:
\begin{align*}
  Seq\trans{x} &= x
  \\
  Seq\trans{\lit} &= \lit
  \\
  Seq\trans{\fn{\ann{x}{\ty}} \expr}
  &=
  \fn{\ann{x}{\ty}} Seq\trans{\expr}
  \\
  Seq\trans{\cn} &= \cn
  \\
  Seq\trans{\ty} &= \ty
\end{align*}
Rather, most of the work of translating Core into Sequent Core is in handling
the apparent computation.  In the compositional translation, apparent
computation expressions all correspond to computation $\mu$-abstractions.
\begin{align*}
  Seq\trans{\expr_1 ~ \expr_2}
  &=
  \comp{\ann r \ty}
    \cut
    {Seq\trans{\expr_1}}
    {\app{Seq\trans{\expr_2}}{r}}
  \\
  &
  \Where{}
    (\expr_1 ~ \expr_2) : \ty
  \\
  Seq\trans{\Let \abind \In \expr}
  &=
  \comp{\ann r \ty}\Let Seq\trans{\abind} \In \cut{\expr}{r}
  \\
  &
  \Where{}
    (\Let \abind \In \expr) : \ty
  \\
  Seq\trans{\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}}
  &=
  \comp{\ann{r}{\ty}}
    \cut
    {Seq\trans{\expr}}
    {\caseas{\ann{x}{\ty}}{\vect[n]{Seq\trans{\analt_n}_{r}}}}
  \\
  &
  \Where{}
    (\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}) : \ty
  \\
  Seq\trans{\coerce{\expr}{\cn}}
  &=
  \comp{\ann{r}{\ty}}\cut{Seq\trans{\expr}}{\koerce{\cn}{r}}
  \\
  &
  \Where{}
    (\coerce{\expr}{\cn}) : \ty
\end{align*}
Because the continuation variable $r$ introduced by the computation abstractions
is annotated with its type, we need to infer the type of all apparent
computations to determine this annotation.  We also need to translate the
binding of a $\Let$ expression and alternatives of a case expression:
\begin{align*}
  Seq\trans{\nonrec{\ann x \ty = \expr}}
  &=
  \nonrec{\bindv{\ann x \ty}{Seq\trans{\expr}}}
  \\
  Seq\trans{\rec{\vect[n]{\ann{x_n}{\ty_n} = \expr_n}}}
  &=
  \rec{\vect[n]{\bindv{\ann{x_n}{\ty_n}}{Seq\trans{\expr_n}}}}
\end{align*}
\begin{align*}
  Seq\trans{\blank \to \expr}_{r}
  &=
  \blank \to \cut{Seq\trans{\expr}}{r}
  \\
  Seq\trans{
    K ~ \vect[n]{\ann{x_n}{\ty_n}} \to \expr
  }_{r}
  &=
  K ~ \vect[n]{\ann{x_n}{\ty_n}}
  \to
  \cut{Seq\trans{\expr}}{r}
  \\
  Seq\trans{\lit \to \expr}_{r}
  &=
  \lit \to \cut{Seq\trans{\expr}}{r}
\end{align*}
Note that because case alternatives in Sequent Core point to self-contained
commands, we explicitly spell out the common continuation that every alternative
``returns'' to, which was introduced as the return continuation for the case
expression itself.  Finally, we can translate a whole program from Core to
Sequent Core by giving some chosen continuation variable on which we expect to
receive the final result of the program:
\begin{align*}
  Seq\trans{\vect[n]{\decl_n}; e}_{r}
  &=
  \vect[n]{\decl_n}; \cut{Seq\trans{e}}{r}
\end{align*}

While the compositional translation is simple, it creates unnecessarily large
terms due to the fact that every apparent computation gets its own
$\mu$-abstraction.  We can then take the observation on the difference between
apparent values and apparent computations in Core to write a better, more
compacting translation into Sequent Core.  This more compacting translation is
closer to the implemented one, but still quite simplified.%
\footnote{The implemented translation handles the renaming necessary to avoid
  static variable capture and also attempts to translate functions which
  represent continuations as continuations (i.e., it performs some
  \emph{re-contification}).}
 %
For apparent values (variables, literals, lambda abstractions, coercions, and
types), the translation is much the same:
\begin{align*}
  Seq\trans{x} &= x
  \\
  Seq\trans{\lit} &= \lit
  \\
  Seq\trans{\fn{\ann{x}{\ty}} \expr}
  &=
  \fn{\ann{x}{\ty}} Seq\trans{\expr}
  \\
  Seq\trans{\cn} &= \cn
  \\
  Seq\trans{\ty} &= \ty
  \\
  Seq\trans{\expr}
  &= \comp{r:\ty} Seq\trans{\expr : \ty} r
  &
  \Where
  \expr &:\ty \text{ is an apparent computation}
\end{align*}
The main difference is when we find an apparent computation (an application, let
expression, case expression, or cast), identified by the last clause above.  In
this case, we introduce \emph{one} $\mu$-abstraction, and then begin to collect
the outer-most continuation and bindings of the computation:
\begin{align*}
  Seq\trans{\expr_1 ~ \expr_2} \ko
  &=
  Seq\trans{\expr_1} (\app{Seq\trans{\expr_2}}{\ko})
  \\[1ex]
  Seq\trans{\Let \abind \In \expr} \ko
  &=
  \Let Seq\trans{\abind} \In Seq\trans{\expr} \ko
  \\
  \Where
    BV(\abind) &\notin FV(\ko)
  \\[1ex]
  Seq\trans{\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}} \ko
  &=
  Seq\trans{\expr} (\caseas{\ann{x}{\ty}}{\vect[n]{Core\trans{\analt_n} \ko}})
  \\
  \Where
  \ko &\text{ is small }
  \\
  Seq\trans{\Case \expr \As \ann{x}{\ty} \Of \vect[n]{\analt_n}} \ko
  &=
  \cut
    {\comp{\ann{r}{\ty'}}
      Seq\trans{\expr}
      (\caseas{\ann{x}{\ty}}{\vect[n]{Core\trans{\analt_n} r}})
    }
    {\ko}
  \\
  \Where
  \ko &:\ty' \text{ is large }
  \\[1ex]
  Seq\trans{\coerce{\expr}{\cn}} \ko
  &=
  Seq\trans{\expr} (\koerce{\cn}{\ko})
  \\[1ex]
  Seq\trans{\expr} \ko
  &=
  \cut{Seq\trans{\expr}}{\ko}
  \qquad
  \Where
    \expr \text{ is an apparent value }
\end{align*}
Some care must still be taken when translating $\Case$ expressions, since we want
to avoid unnecessary duplication of large continuations inside the branches of
alternatives.  For now, we just check if the continuation to a $\Case$ is small
enough to duplicate, and if not, bind it with a computation abstraction.  When
we reach an apparent value again, in the last clause above, we write down the
entire continuation and list of bindings found during translation.

The more compacting translation of bindings, alternatives, and whole programs
are effectively the same as before, except that for a binding we begin expecting
the bound expression to be value-like, and for an alternative and whole program
we begin expecting the resulting expression to be computation-like:
\begin{align*}
  Seq\trans{\nonrec{\ann x \ty = \expr}}
  &=
  \nonrec{\bindv{\ann x \ty}{Seq\trans{\expr}}}
  \\
  Seq\trans{\rec{\vect[n]{\ann{x_n}{\ty_n} = \expr_n}}}
  &=
  \rec{\vect[n]{\bindv{\ann{x_n}{\ty_n}}{Seq\trans{\expr_n}}}}
\end{align*}
\begin{align*}
  Seq\trans{\blank \to \expr} \ko
  &=
  \blank \to Seq\trans{\expr} \ko
  \\
  Seq\trans{K ~ \vect[n]{\ann{x_n}{\ty_n}} \to \expr} \ko
  &=
  K ~ \vect[n]{\ann{x_n}{\ty_n}}
  \to
  Seq\trans{\expr} \ko
  \\
  Seq\trans{\lit \to \expr} \ko
  &=
  \lit \to Seq\trans{\expr} \ko
\end{align*}
\begin{align*}
  Seq\trans{\vect[n]{\decl_n}; \expr}{r}
  &=
  \vect[n]{\decl_n}; Seq\trans{\expr} r
\end{align*}

\subsection{From Sequent Core to Core}

Translating Sequent Core terms back into Core expressions is also fairly
straightforward, given by $Core\trans{\tm}$:
\begin{align*}
  Core\trans{x} &= x
  \\
  Core\trans{\comp{\ann r \ty} \cmd} &= Core\trans{\cmd}_{r}
  \\
  Core\trans{\fn{\ann x \ty}\tm} &= \fn{\ann x \ty}Core\trans{\tm}
  \\
  Core\trans{\lit} &= \lit
  \\
  Core\trans{\cn} &= \cn
  \\
  Core\trans{\ty} &= \ty
\end{align*}
All but one Sequent Core term, namely the computation abstraction, corresponds
directly to a Core value.  To translate a computational term to a Core
expression, we need to translate the underlying command and read off the result
returned to the abstracted continuation variable, as given by
$Core\trans{\cmd}_{r}$:
\begin{align*}
  Core\trans{\Let \abind \In \cmd}_{r}
  &=
  \Let Core\trans{\abind}_{r} \In Core\trans{\cmd}_{r}
  \\
  Core\trans{\cut{\tm}{\ko}}_{r}
  &=
  Core\trans{\ko}_{r}[Core\trans{\tm}]
  \\
  Core\trans{\jump{\vect[n]{\tm_n}}{j}}_{r}
  &=
  j ~ \vect[n]{Core\trans{\tm_n}}
\end{align*}
A $\Let$-binding translates to a $\Let$-binding, a command $\cut{v}{k}$
translates to evaluating expression corresponding to the term $v$ inside the
evaluation context corresponding to the continuation $k$, and a jump
$\jump{\vect{v}}{j}$ translates to a tail function call (more on this later).
The evaluation contexts corresponding to continuations which return their result
on $r$ are given by $Core\trans{k}_{r}$:
\begin{align*}
  Core\trans{\ret r}_{r} &= \hole
  \\
  Core\trans{\app{\tm}{\ko}}_{r}
  &=
  Core\trans{\ko}_{r}[\hole ~ Core\trans{\tm}]
  \\
  Core\trans{\koerce{\cn}{\ko}}_{r}
  &=
  Core\trans{\ko}_{r}[\coerce{\hole}{\cn}]
  \\
  Core\trans{\caseas{\ann x \ty}{\vect[n]{\analt_n}}}_{r}
  &=
  \Case \hole \As \ann x \ty \Of \vect[n]{Core\trans{\analt_n}_{r}}
\end{align*}

Besides translating terms, commands, and continuations, we also need to convert
Sequent Core bindings and alternatives back into Core bindings and alternatives.
The alternatives of case analysis have a straightforward, one-to-one
correspondence between Core and Sequent Core, given by $Core\trans{\analt}_{r}$,
where Sequent Core alternatives lead to commands that output their result on a
return continuation variable $r$:
\begin{align*}
  Core\trans{\blank \to \cmd}_{r}
  &=
  \blank \to Core\trans{\cmd}_{r}
  \\
  Core\trans{K ~ \vect[n]{\ann x \ty} \to \cmd}_{r}
  &=
  K ~ \vect[n]{\ann x \ty} \to Core\trans{\cmd}_{r}
  \\
  Core\trans{\lit \to \cmd}_{r}
  &=
  \lit \to Core\trans{\cmd}_{r}
\end{align*}
Translating the bindings back to Core is more interesting, since Sequent Core
has two different types of bindings (value and continuation bindings) whereas
Core only has the one.  Thus, we collapse both bound terms and continuations
into bound Core expressions, given by $Core\trans{\abind}_{r}$ and
$Core\trans{\bp}_{r}$.
\begin{align*}
  Core\trans{\nonrec{\bp}}_{r}
  &=
  \nonrec{Core\trans{\bp}_{r}}
  \\
  Core\trans{\rec{\vect[n]{\bp_n}}}_{r}
  &=
  \rec{\vect[n]{Core\trans{\bp_n}_{r}}}
\end{align*}
\begin{align*}
  Core\trans{\bindv{\ann x \ty}{\tm}}_{r}
  &=
  (\ann x \ty = Core\trans{\tm})
  \\
  Core\trans{
    \bindk{\ann j \ty}{\fnk{\vect[n]{\ann{x_n}{\ty_n}}}\cmd}
  }_{r}
  &=
  (
    \ann{j}{Core^\neg\trans{\ty}_{\ty'}}
    =
    \vect[n]{\fn{\ann{x_n}{\ty_n}}} Core\trans{\cmd}_{r}
    )
  \\
  \Where
  r &: \ty'
\end{align*}
We need to know the return variable $r$ in order to convert continuation
bindings, since continuations do not just return results, but instead output
them on the continuation variable $r$.  In the conversion of the continuation
bound to $j$ into an expression, we effectively negate the type of $j$.  So
instead of standing for a continuation that accepts multiple inputs as an
unboxed tuple, the converted $j$ stands for a function from the multiple inputs
to the ultimate result type given by the surrounding return continuation $r$.
The negation of a parameterized continuation is given by
$Core^\neg\trans{\sty}_{\ty}$, where $\ty$ is the ultimate return type of the
continuation:
\begin{align*}
  Core^\neg\trans{\unitjmpty}_{\ty} &= \ty
  \\
  Core^\neg\trans{\prodjmpty{\ty'}{\sty}}_{\ty}
  &=
  \ty' \to Core^\neg\trans{\sty}_{\ty}
  \\
  Core^\neg\trans{\existjmpty{\ann a \ki} \sty}_{\ty}
  &=
  \forall \ann a \ki. Core^\neg\trans{\sty}_{\ty}
\end{align*}
We can read the translation $Core^\neg\trans{\sty}_{\ty}$ as a logical negation
of $\sty$ if we understand the result type $\ty$ to stand in for falsehood.  For
instance, $\unitjmpty$ is logically interpreted as ``true,'' and so its negation
is the ``false'' return type $\ty$.  The logical negation of ``there exists an
$a$ such that $\sty$ is true'' is ``for all $a$, $\sty$ is false,'' hence the
third clause.  The middle clause for converting a product type into a function
type is a bit more indirect form of negation.  However, it follows from the
familiar De Morgan's laws if we (informally) understand the function type
$\ty \to \sty$ as a form of disjunction $(\neg \ty) \vee \sty$:
\begin{align*}
  \neg(\ty \wedge \sty)
  &=
  (\neg \ty) \vee (\neg \sty)
  =
  \ty \to (\neg \sty)
\end{align*}
Hence, the negated interpretation of the product jump type,
$Core^\neg\trans{\prodjmpty{\ty'}{\sty}}_{\ty}$, is a function,
$\ty' \to Core^\neg\trans{\sty}_{\ty}$.

To translate the whole Sequent Core program back into Core, we just translate
the main command with respect to the continuation on which we expect to receive
the program's result.
\begin{align*}
  Core\trans{\vect[n]{\decl_n}; \cmd}_{r}
  &=
  \vect[n]{\decl_n}; Core\trans{\cmd}_{r}
\end{align*}

\section{Design discussion}
\label{sec:design-discussion}

\subsection{Join points}

One of the goals of Sequent Core is to provide a good representation for naming
join points in a program.  A \emph{join point} is a specified point in the
control flow of a program where to different possible execution paths join back
up again.  The purpose of giving a name to these join points is to avoid
excessive duplication of code and keep code size small.  Since a join point
represents the future execution path of a program, we would like to model them
as continuations, and so named join points become named continuations.

The main place where new join points are named are in the branches of a case
expression.  For example, consider the Core expression
\begin{align*}
  e &= \Case e_0 \Of \{ K_1 ~ x ~ y \to e_1; K_2 \to e_2 \}
\end{align*}
In Sequent Core, $e$ would be represented as the term $v$:
\begin{align*}
  v
  &=
  \comp{r}
    \cut
    {\comp{q}\cmd_0}
    {\Case\Of \{ K_1 ~ x ~ y \to \cmd_1; K_2 \to \cmd_2 \}}
\end{align*}
where the term $\comp{q}\cmd_0$ corresponds to the expression $e_0$, and the
commands $\cmd_1$ and $\cmd_2$ correspond to the expressions $e_1$ and $e_2$ run
in the context of the continuation $r$ which expects the end result returned by
$e$.  Now, it would be semantically correct to inline the case continuation for
$q$ inside of $\cmd_0$, which could open up further simplification.  For
example, suppose that $\cmd_0$ is:
\begin{align*}
  \cmd_0
  &=
  \cut{z}{\Case\Of \{ True \to \cut{K_1}{\app{5}{\app{10}{q}}}; False \to \cut{z'}{q} \}}
\end{align*}
Inlining for $q$ everywhere in $\cmd_0$ then corresponds to the case-of-case
transformation performed by GHC on Core expressions.

However, inlining for $q$ everywhere can duplicate the commands $\cmd_1$ and
$\cmd_2$, leading to larger code size!  We only want to inline selectively in
places where we are confident that inlining the case continuation would lead to
further simplification, but not elsewhere.  We can achieve selective inlining by
assigning the continuation to its name, $q$:
\begin{align*}
  v
  &=
  \comp{r}
  \Let q = \Case\Of \{ K_1 ~ x ~ y \to \cmd_1; K_2 \to \cmd_2 \}
  \In \cmd_0
\end{align*}
and then follow heuristics to inline for $q$ only in those places that matter.
In particular, we could just inline inside of the one branch in $\cmd_0$ which
provides $q$ with a constructed result, leading to the command:
\begin{align*}
  \cmd_0
  &=
  \cut{z}{\Case\Of \{ True \to \cmd_1[5/x,10/y]; False \to \cut{z'}{q} \}}
\end{align*}
In effect, selective inlining has eliminated case analysis on a known case by
duplicating the command $\cmd_1$, but not duplicating $\cmd_2$.

There is still a problem, though: what if the command $\cmd_1$ is so large that
this simplification is not worth the duplication?  In that case, we would like
to form a \emph{new} join point that gives a name to the command $\cmd_1$, so
that we may refer to it by name rather than duplicating the entire command.
Since $\cmd_1$ is an open command referring to local variables introduced by the
case analysis, we must first abstract over these local variables.  This is the
role of the polyadic form of continuation, which is able to receive multiple
inputs before running.  In our example, we would move $\cmd_1$ and $\cmd_2$ into
a named, polyadic continuations:
\begin{gather*}
\begin{aligned}
  v
  &=
  \comp{r}
  &
    \Let
      p_1 &= \fnk{\pack{x}{y}} \cmd_1;
  \\
  &&
      p_2 &= \fnk{\packs{}{}} \cmd_2;
  \\
  &&
      q &= \Case\Of \{
          K_1 ~ x ~ y \to \cut{\pack{x}{y}}{p_1};
          K_2 \to \cut{\packs{}{}}{p_2}
          \}
  \\
  &&
    \In
    \cmd_0
\end{aligned}
\end{gather*}
Now, $q$ stands for a small continuation, and so it can be inlined much more
aggressively.  For example, we can perform a more lightweight inlining into
$\cmd_0$:
\begin{align*}
  \cmd_0
  &=
  \cut{z}{\Case\Of \{ True \to \cut{\pack{5}{10}}{p_1}; False \to \cut{z'}{q} \}}
\end{align*}
which avoids duplicating $\cmd_1$ at all.

\subsection{Polymorphic-polyadic continuations and the existential stack}

Due to the fact that some constructors to data types contain existentially
quantified types, it is important that we be able to abstract over a command
with free type variables as a polymorphic continuation.  Thus, the more general
form of the $\lambda$-continuation may introduce several type variables in
addition to term variables.  This means that the stacks which are provided to
these continuations are effectively a form of existential tuple.  Currently, we
represent these polymorphic continuations and continuations expecting multiple
inputs as an uncurried $\lambda$-abstraction.  However, there are multiple other
design choices which are possible for this purpose:
\begin{itemize}
\item Unboxed tuples: Morally, the stack $\pack{1}{\pack{2}{3}}$ for a polyadic
  continuation is the same as the unboxed tuple $(\# 1, 2, 3 \#)$.  Why then do
  we bother to introduce a new form of term, and a new type, instead of just
  using unboxed tuples for this purpose?  The reason is the above-mentioned
  existentially-quantified types, which lie outside the form of unboxed tuples
  supported by Core.  We cannot represent
  $\packrets[\stackty{a:\star}{a, a, a}]{\pack{Int}{\pack{1}{\pack{2}{3}}}}$ as
  an unboxed tuple.

  As a minor advantage, having the special form for existential stacks lets us
  (actually, requires us to) translate them back into Core as a calling context
  rather than a value, so $\cut{\pack{1}{\pack{2}{3}}}{q}$ becomes $q ~ 1 ~ 2 ~ 3$
  instead of $q (\# 1, 2, 3 \#)$.  This results in Core expressions for join
  points looking closer to what the current GHC simplifier actually produces.

\item Curried vs uncurried: Currently, the continuations which accept
  existential stacks are written in a totally-uncurried form.  By listing all
  the parameters that abstract over a command, we are forced to spell out
  \emph{exactly} the number of inputs that are required by the continuation
  before any work can be done.

  An alternative design would be to give a curried form of continuations which
  accept existential stacks.  These would have one of two forms:
  \begin{itemize}
  \item $\fnk{\ann x \ty}k$: accept the first element of the stack, which is a
    term of type $\ty$, as $x$, then pass the rest of the stack to $k$ (i.e.,
    pop the top element of the stack as $x$ and then continue as $k$)
  \item $\fnk{\ann a \ki}k$: accept the first element of the stack, which is a
    type of kind $\ki$, as $a$, and then pass the rest of the stack to $k$
    (i.e., specialize the type variable $a$ for the continuation $k$)
  \end{itemize}
  which could be collapsed like ordinary $\lambda$-abstractions.  To model a
  unary continuation, like the continuation which accepts the ``end'' of the
  stack, we could introduce the dual to general computation abstractions.  These
  are continuations of the form $\letin{\ann x \ty}\cmd$ which accept an input
  named $x$ before performing some arbitrary computation $\cmd$, and correspond
  to the context $\Let x:\ty = \hole \In \expr$ in Core.  However, these
  fundamentally introduce \emph{non-strict} continuations, which is a whole can
  of worms we have been avoiding thus far.  In the end, it may be worthwhile to
  introduce these general input continuations for independent reasons, but we
  leave them out for now.

\item Case alternative: Instead of introducing a special new form of
  continuations for accepting existential stacks, we could also have added a
  form of case alternative for matching on the stack.  That way the special
  continuations $\fnk{\packs{\vect{\ann a \ki}}{\vect{\ann x \ty}}}\cmd$ would
  just be written as another form of case continuation
  $\Case\Of{\packs{\ann a \ki}{\ann x \ty} \to \cmd}$.  This unification might
  make sense following the story that existential stacks are just a sort of
  unboxed data type that are not available in Core, so it makes sense that we
  would pattern match on them.

  For now we don't fold continuations for existential stacks, but it is unclear
  if doing so would be a benefit (one less type of continuation to consider) or
  a hinderance (if they need to be treated differently in enough of the compiler
  that we effectively special case them anyways).  It is worth keeping this
  alternative design in mind, to evaluate which of the two options would pan out
  better in practice.
\end{itemize}

\subsection{Continuation-closed terms}

Currently, we make the scope and type restriction that forces all terms to never
reference free continuation variables.  This kind of restriction is necessary to
ensure that Sequent Core programs correspond directly to Core programs, and do
not introduce new kinds of control flow that would require a full
continuation-passin style (CPS) translation back into Core.  However, it is
probably sufficient to impose this scoping restriction on \emph{values} (and in
particular, $\lambda$-abstractions).  This would allow for general computation
terms ($\mu$-abstractions $\comp{q}\cmd$) to reference continuations in its
scope in order to produce a (continuation-closed) value.  Since general
computations cannot escape their scope, due to call-by-need semantics, this
should be fine and should not require a full CPS translation back into Core.

The scope restriction on general computation terms comes up during translation
from Core to Sequent Core.  Currently, the restriction forces the continuation
representing the context of a case expression be let-bound, rather than use a
$\mu$-abstraction.  This forces more continuations to be abstracted into the
local let bindings, which in effect introduces more named join points back in
Core world.  This is probably not bad, since we want to be introducing join
points anyway, but it is something to keep in mind.

\end{document}
